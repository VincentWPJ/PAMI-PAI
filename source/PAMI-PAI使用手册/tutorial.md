# PAMI-PAI使用手册

大家好，欢迎使用227集群——PAMI-PAI！

PAMI-PAI集群采用的管理工具为 Determined AI。这是一个开源的服务器集群化解决方案。之前的本地服务器将转换为集群的计算节点，另外集群内配置有约100T的raid5机械硬盘阵列和8T的高速固态。这里有几点注意事项：

1、集群内有各种型号的显卡（目前有两个计算节点，分别有两张1080ti和两张3090），目前没有限制用户的卡数，理论上一个人可以占用集群全部显卡资源。这是考虑到在赶论文ddl时，大家对卡的需求会很高。但平时使用时，请大家合理申请显卡资源，大家都是需要显卡资源的！

2、集群内的网络配置了全球加速，是专为下载或拉取科研相关资料的，禁止通过集群网络传播非法信息！

3、因为集群的维护很耗费时间和精力，所以初期我只能提供大家最基本的用法（可以满足正常科研需求），至于更多的用法和问题希望同学们群策群力，一起完善。我专门为集群使用写了网页文档： https://pami-pai.readthedocs.io/en/latest/ 。之后这个文档我会尽量保证每周更新，会把同学们在群里讨论的内容整理到Q&A部分，方便以后新进入的同学寻找答案！

4、集群最大的作用是隔离各个用户的使用环境，所以使用集群的同学请先学习docker相关知识。跑代码时先在本地构建好实验环境，然后将环境镜像直接迁移至集群即可（官方提供了基础镜像，一般只需稍作修改，官方的镜像地址为：https://hub.docker.com/r/determinedai/environments/tags ）。

5、申请到显卡资源后，请大家充分使用。不要空占显卡！（后期也许会增加脚本定期扫描显卡占用情况，用以kill掉显卡状态为空闲的任务）

6、初期阶段的集群维护人员为：weipengjin， fangkun， liuzhaojiang。

## PAMI-PAI基本介绍
### 计算节点
计算节点由实验室之前零散的GPU服务器组成，计算节点的配置各不相同，性能也有差异，可以针对自己的任务申请合适的显卡资源。相对来说含有新款显卡（3090）的节点会比较热门，大家合理选择。
### 存储节点
存储节点为整个集群提供存储服务。存储节点含有7 * 16T的机械硬盘，4 * 2T的固态硬盘。机械硬盘是raid5阵列，数据有安全冗余，而固态硬盘则采用raid0阵列，最大化使用空间。在各个计算节点上 /home/tpami/pai-hdd 对应的是机械存储，/home/tpami/pai-cache 对应的是固态存储。因为阵列的存在和带宽的限制，两者的顺序读写性能相差不大，但随机读写的性能差距很大。所以尽量将数据集放在pai-cache中，而将实验log等放在pai-hdd中。
#### 存储节点硬件配置
存储节点的配置为：
cpu：13700k
主板：技嘉超级雕（带万兆口）
内存：128G DDR4 3200
固态硬盘：8T
机械硬盘：7 * 16T 企业级
散热器：利民FS140
电源：航嘉650w金牌全模组电源
扩展卡：sata扩展卡，
机箱：多盘位eatx机箱
系统：truenas
#### 存储性能测试
实验室内网目前已全部升级为千兆，亲测文件传输速率平均可以达到90M/s，至于为何有20%左右的性能损耗，目前暂没有时间排查。按照目前初步使用情况来看，千兆内网已能够满足需求。实验室已购买万兆交换机，如后期集群在使用中出现带宽瓶颈，则会将所有节点升级为万兆网卡。

#### 本地挂载文件服务器
此举用于上传自己的实验代码和数据到集群，同时也用于从集群中下载训练模型或log文件。
以我自己的电脑为例，大家使用时只需修改后面的本地挂载路径就行
```
# 挂载固态硬盘（用于存放数据集）
sudo mount -t nfs 192.168.50.60:/mnt/pai-cache/pai-cache  /home/wpj/pai-cache
# 挂载机械阵列（用与存放实验代码）
sudo mount -t nfs 192.168.50.60:/mnt/pai-hdd/pai-hdd  /home/wpj/pai-hdd
```

## 登录集群
以本地Ubuntu系统为例（机器在227，且连入227的内网）：
```
# 添加环境变量
vim ~/.bashrc
# 添加如下内容：
export DET_MASTER=192.168.50.90
# 执行修改
source ~/.bashrc
# 登录
det user login
username: xxxx
password for user 'xxxx':
```
对于机器不在227，如在418或校内其他地方的同学，仅需修改环境变量
```
export DET_MASTER=202.120.37.202
```
## 提交作业
集群的显卡资源申请：
```
# 按照config.yaml文件所述申请显卡，并打开命令行
det shell start --config-file config.yaml
```
config文件模板：
```
description: wpj_task
resources:
  agent_label: 1080ti
  slots: 1
bind_mounts:
  - host_path: /home/tpami/pai-hdd/weipengjin
    container_path: /run/determined/workdir/weipengjin
environment:
  image: determinedai/environments:cuda-11.1-pytorch-1.9-lightning-1.3-tf-2.4-gpu-0.16.3
```
description: 任务名称
agent_lable: 显卡型号
slot: 显卡数量（目前仅支持单节点多卡的形式，不支持多节点多卡的形式）
host_path: 主机挂载路径（统一格式为：/home/tpami/pai-hdd/username）
container_path: 容器内挂载路径（随意）
image: 官方基础镜像或自己制作的镜像（自己制作的镜像建议以官方基础镜像为base）

## 查看集群显卡状态
```
# 查看集群下面所有节点。将返回节点的ip，显卡数量，显卡标签等
det -m 192.168.50.90 agent list
```
```
# 查看所有节点下显卡的状态。返回的信息包括显卡状态、任务对应的容器代号、占用显卡的任务名称等
det slot list
```

## 查看作业日志
可以通过 http://202.120.37.202:8080 登录集群的网页端查看当前集群的状态和任务的log。
## 账号申请
集群账号由杨杰、刘伟两位老师负责管理、发放、回收。
## Q&A
